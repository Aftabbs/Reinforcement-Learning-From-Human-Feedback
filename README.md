# Reinforcement-Learning-From-Human-Feedback

![image](https://github.com/user-attachments/assets/31034415-bad5-4947-9724-1172b5c0150c)

Reinforcement learning from human feedback (RLHF) is a machine learning (ML) technique that uses human feedback to optimize ML models to self-learn more efficiently. Reinforcement learning (RL) techniques train software to make decisions that maximize rewards, making their outcomes more accurate. RLHF incorporates human feedback in the rewards function, so the ML model can perform tasks more aligned with human goals, 
wants, and needs. RLHF is used throughout generative artificial intelligence (generative AI) applications, including in large language models (LLM).


![image](https://github.com/user-attachments/assets/c478b9f3-df1e-42e2-9007-5bd79c1fe60f)
